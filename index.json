[{"authors":["joe"],"categories":null,"content":"Joe Gardiner is an experienced technical architect working primarily in a pre-sales function for a number of vendors and MSPs. His background spans product management, business unit management, pre-sales solution architecting, and technical evangelism.\nHe has worked with a range of technologies, from infrastructure products to database and SaaS solutions, and has successfully applied this experience when working on projects across a range of industries such as finance, manufacturing, retail, and pharmaceutical.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"e85fc592a209a43e7837518f8ac1565c","permalink":"https://grdnrio.github.io/grdnr-live-site/author/joe-gardiner/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/grdnr-live-site/author/joe-gardiner/","section":"authors","summary":"Joe Gardiner is an experienced technical architect working primarily in a pre-sales function for a number of vendors and MSPs. His background spans product management, business unit management, pre-sales solution architecting, and technical evangelism.","tags":null,"title":"Joe Gardiner","type":"authors"},{"authors":["admin"],"categories":null,"content":"Nelson Bighetti is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed neque elit, tristique placerat feugiat ac, facilisis vitae arcu. Proin eget egestas augue. Praesent ut sem nec arcu pellentesque aliquet. Duis dapibus diam vel metus tempus vulputate.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://grdnrio.github.io/grdnr-live-site/author/nelson-bighetti/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/grdnr-live-site/author/nelson-bighetti/","section":"authors","summary":"Nelson Bighetti is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.","tags":null,"title":"Nelson Bighetti","type":"authors"},{"authors":null,"categories":null,"content":"Docker makes running containers incredibly simple, a big reason for its popularity. I can quickly and easily run an Nginx container on my workstation, whether Mac, Windows or Linux based.\ndocker container run --publish 80:80 --detach --name nginx nginx\nAnd as if my magic\u0026hellip;\nThe certified images from Docker are great, and if you use Docker Cloud you can push the images through a pipeline that executes security / vulnerability scanning on the image. However, there are plenty of images in the registry from community contributors - how can you verify them? What sit he image contains software that needs to be patched?\nWe can use InSpec, the compliance testing project from Chef, to verify the state of our Docker images against a security or compliance baseline. You can read more about the InSpec language and the resources available for writing tests on the homepage, or in my blog on the subject.\nPlease note, at the moment this will not work on Windows containers\nInSpec setup You can install InSpec as part of the ChefDK or by grabbing the executable from the downloads page.\nOnce installed, reload your terminal session and you should have the inspec CLI in your path.\nC:\\Users\\jgard\u0026gt; inspec --help Commands: inspec archive PATH # archive a profile to tar.gz (default) ... inspec artifact SUBCOMMAND ... # Sign, verify and install artifacts inspec check PATH # verify all tests at the specified PATH inspec compliance SUBCOMMAND ... # Chef Compliance commands inspec detect # detect the target OS inspec env # Output shell-appropriate completion co... inspec exec PATHS # run all test files at the specified PATH. inspec habitat SUBCOMMAND ... # Commands for InSpec + Habitat Integration inspec help [COMMAND] # Describe available commands or one spe... inspec init TEMPLATE ... # Scaffolds a new project inspec json PATH # read all tests in PATH and generate a ... inspec shell # open an interactive debugging shell inspec supermarket SUBCOMMAND ... # Supermarket commands inspec vendor PATH # Download all dependencies and generate... inspec version # prints the version of this tool Options: l, [--log-level=LOG_LEVEL] # Set the log level: info (default), debug, warn, error [--log-location=LOG_LOCATION] # Location to send diagnostic log messages to. (default: STDOUT or STDERR) [--diagnose], [--no-diagnose] # Show diagnostics (versions, configurations)  Using the inspec CLI, we can execute scans against local or remote machines. For example the following will execute a profile against the local machine.\ninspec exec /path/tp/profile/linux_baseline\n\u0026hellip; and this can be used for remote machines.\ninspec exec /path/tp/profile/linux_baseline -t ssh://1.2.3.4\nNote we\u0026rsquo;re not setting any additional options, so check the inspec exec help for more information.\nBoth examples assume we have a profile on our local machine for scanning purposes. There are loads of open source profiles available on the Dev-Sec project\u0026rsquo;s GitHub page. You can clone a profile or just grab the archive. The key thing is that the profile follows the skeleton format so the InSpec CLI can interpret it correctly.\nScanning a container I\u0026rsquo;m going to demonstrate a simple scan against the latest Windows Server Core image from the registry. First let\u0026rsquo;s run the container.\ndocker container run --detach -i --name ubuntu ubuntu\nCheck it\u0026rsquo;s running.\nroot@ip-172-31-47-31:~/profiles# docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 6242a0d510c1 ubuntu \u0026quot;/bin/bash\u0026quot; 9 minutes ago Up 9 minutes ubuntu  Now let\u0026rsquo;s grab an InSpec profile to run against this container.\nroot@ip-172-31-47-31:~/profiles git clone https://github.com/dev-sec/linux-baseline.git Cloning into 'windows-baseline'... remote: Counting objects: 206, done. remote: Total 206 (delta 0), reused 0 (delta 0), pack-reused 206R Receiving objects: 100% (206/206), 39.44 KiB | 0 bytes/s, done. Resolving deltas: 100% (100/100), done.  Using the InSpec CLI I can now run the profile against the Docker container passing in the path to the Linux baseline I just cloned and setting the Docker container ID as a target.\ninspec exec linux-baseline -t docker://6242a0d510c1\nHere\u0026rsquo;s an example of an output against the official Ubuntu image.\n--- SOME TEST OUPUT --- ✔ sysctl-31a: Secure Core Dumps - dump settings ✔ Kernel Parameter fs.suid_dumpable value should cmp == /(0|2)/ × sysctl-31b: Secure Core Dumps - dump path (expected \u0026quot;|/usr/share/apport/apport %p %s %c %P\u0026quot; to match /^\\/.*/ Diff: @@ -1,2 +1,2 @@ -/^\\/.*/ +\u0026quot;|/usr/share/apport/apport %p %s %c %P\u0026quot; ) × Kernel Parameter kernel.core_pattern value should match /^\\/.*/ expected \u0026quot;|/usr/share/apport/apport %p %s %c %P\u0026quot; to match /^\\/.*/ Diff: @@ -1,2 +1,2 @@ -/^\\/.*/ +\u0026quot;|/usr/share/apport/apport %p %s %c %P\u0026quot; ✔ sysctl-32: kernel.randomize_va_space ✔ Kernel Parameter kernel.randomize_va_space value should eq 2 ✔ sysctl-33: CPU No execution Flag or Kernel ExecShield ✔ /proc/cpuinfo Flags should include NX Profile Summary: 22 successful, 30 failures, 1 skipped Test Summary: 62 successful, 56 failures, 1 skipped  Wrapping up In the example above I\u0026rsquo;m using a linux-baseline profile meant for complete Linux OS\u0026rsquo;, not an Ubuntu based container. Having said that the principle is still incredibly relevant in a container based workload environment.\nInSpec allows us to test the output of a Docker container build, essentially define integration tests for containers\u0026hellip; it\u0026rsquo;s just a matter of designing the tests!\nUsing the InSpec CLI this can simply for part of a CI/CD pipeline, with a build node calling the InSpec CLI against a dynamic Docker target (container ID).\n","date":1502712431,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1502712431,"objectID":"2508c8f4782f731a2690667b9dc5fa53","permalink":"https://grdnrio.github.io/grdnr-live-site/post/2017-08-14-security-scanning-docker-containers-with-inspec/","publishdate":"2017-08-14T13:07:11+01:00","relpermalink":"/grdnr-live-site/post/2017-08-14-security-scanning-docker-containers-with-inspec/","section":"post","summary":"Docker makes running containers incredibly simple, a big reason for its popularity. I can quickly and easily run an Nginx container on my workstation, whether Mac, Windows or Linux based.","tags":["inspec","security","docker"],"title":"Security scanning Docker containers with InSpec","type":"post"},{"authors":null,"categories":null,"content":"Have you heard of GDPR? It stands for the General Data Protection Regulation. It\u0026rsquo;s an update to the original data protection regulations from 1995 and reflects the new ways that we store and process personal data in an increasingly digital society. It impacts any business that stores and processes the personal data of EU citizens (whether EU based or not), and introduces a number of requirements relating to access to data, breach notifications, and the right to be forgotten.\nShould an organisation subject to GDPR fail to meet the requirements and fail an audit, it will be subject to financial penalties. These can be up to €20 million, or 4% of global annual turnover, whichever is greater.\nWhilst GDPR undoubtedly introduces a number of new requirements for an organisation as a whole, the specific technical controls for an IT function are not defined beyond a baseline of best practises. What is mandated however is the idea of \u0026lsquo;Privacy by Design\u0026rsquo;. GDPR requires that all new projects involving the creation of new data processes should have privacy considerations included in the design stage and implemented at the core. This introduces a range of challenges relating to auditing at scale, audit schedules and, most importantly, audit integration with the processes being designed.\nCompliance as code If we\u0026rsquo;re building new IT processes, often redefining existing processes such as infrastructure deployment in code (Terraform, CFT, ARM templates etc), we can do the same with our compliance controls. InSpec allows us to do this.\nIt\u0026rsquo;s an open source project from Chef; very high level and readable, reflects the security control structure many InfoSec orgs are used to, and be executed against systems in a number of ways.\nIf added a few examples that are relevant to GDPR, bearing in mind the requirement for data privacy / security.\nExamples Here\u0026rsquo;s an example for some baseline MySQL hardening.\n Another example, this time of workstation hardening.\n Finally an example of checking firewall config.\n Profiles InSpec lets us collect these code blocks, each representing a security control, into profiles. The profiles align to our business or regulatory requirements. For example, we can build a profile that combines GDPR with our business\u0026rsquo; baseline security configuration- think domain joining, password length or system entropy.\nYou can check out a couple of baseline on the Dev-Sec GitHub org.\n  Windows baseline profile  Linux baseline profile  I\u0026rsquo;ll add a link to a GDPR profile when it\u0026rsquo;s available.\nInSpec Automation InSpec profiles can be executed against a target system in a number of ways. Whichever method you decide upon requires the installation of the InSpec Gem (on the target system or your workstation if you decide to go with remote execution).\nIt\u0026rsquo;s worth noting the Audit cookbook that can be executed via the Chef agent. This forms a natural part of a Chef based approach to system configuration. However, you may also wih to use the gem standalone, or execute target systems from the InSpec CLI. More important than the execution method is the use case.\nPipeline execution A non-Chef based pipeline would suit the local execution method. You could pull a profile hosted on GitHub or similar to a target system and run InSpec as follows, or execute against a remote machine from an admin node or similar. This certainly demonstrates the GDPR requirement for privacy by design for any application deployment pipeline.\ninspec exec --host=1.2.3.4 --profiles-path=/etc/inspec/profile/my-GDPR-profile --format=json\nNote the ouptut format. This is really useful if you want to forward results to something like Splunk.\nInfrastructure deployment Automated infrastructure deployment, on cloud or on-prem, offers a great opportunity to build GDPR auditing in from the start. There are many methods available, but user-data is common. We can used user-data to execute InSpec, similar to the above, or bootstrap the system with Chef for agent based execution. The following assumes the latter.\nUser data to bootstrap is available on the Chef Docs.\nPlease note that you\u0026rsquo;ll need to set the audit cookbook in the node\u0026rsquo;s run-list, either directly or with a base role for example.\nOnce bootstrapped you\u0026rsquo;ll need to set some attributes for the Audit cookbook to enable audit scanning.\n The above example could be set in a role or via the UI if you prefer. You\u0026rsquo;ll see we\u0026rsquo;re setting a profile located on GitHub. This will be downloaded to the target machine as part of the Chef run. Data is being reported directly to the Chef Automate platform.\nFurther information There are many more examples for InSpec and GDPR scanning, such as container based scenarios or as part of a server migration process. I\u0026rsquo;ll continue to add blogs with more detail as GDPR technical controls become clearer and integration points develop.\nIn the meantime I highly recommend the Chef InSpec tutorials for further learning.\n","date":1502538237,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1502538237,"objectID":"4a849745f531b21f00450c9e6a895e5a","permalink":"https://grdnrio.github.io/grdnr-live-site/post/2017-08-12-using-inspec-to-devops-gdpr-compliance/","publishdate":"2017-08-12T12:43:57+01:00","relpermalink":"/grdnr-live-site/post/2017-08-12-using-inspec-to-devops-gdpr-compliance/","section":"post","summary":"Have you heard of GDPR? It stands for the General Data Protection Regulation. It\u0026rsquo;s an update to the original data protection regulations from 1995 and reflects the new ways that we store and process personal data in an increasingly digital society.","tags":["inspec","security"],"title":"Using InSpec to DevOps GDPR compliance","type":"post"},{"authors":null,"categories":null,"content":"Enforcing security policy is tough, especially in a Windows environment where you are NOT using Group Policy. Think about usage patterns for Windows server on cloud. GPO certainly doesn’t always apply. In fact, Microsoft actually recommend GPO for Desktop / Workstation use and other solutions, such as Chef and DSC for servers.\nSo then, how should one manage Local security policy on servers, at scale? Fortunately there’s a decent automation tool you may have heard of… Chef! I wrote a cookbook for managing local security policy. It offers the following features:\n Idempotent execution of policy via secedit.exe Exporting of security databases Import and configure options Custom security databases Security policy generation via template  Take a look at this example custom resource wrapper for secedit.exe.\n Using this cookbook This is a helper cookbook so you need to make sure you add it to your metadata file in another cookbook. A good example is the Dev-Sec project’s Windows Hardening cookbook.\nThis cookbook establishes a baseline of hardening on Windows Servers (2012 through to Nano).\nCheck out the metadata file, it includes my security policy cookbook.\n By including this cookbook, you then have access to the resources. You may want to use them as follows in this example in any normal recipe you may write.\n What’s next? This is an open source project, so I’d love your contributions. I’m working on a custom resource for managing Audit Policy in the next release. If you’d like to get involved… or report a bug… check out the GitHub project.\n View the complete cookbook\n","date":1501753497,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1501753497,"objectID":"8ac41cd6b6c07fc74ffcb4781a64621b","permalink":"https://grdnrio.github.io/grdnr-live-site/post/2017-08-03-automating-windows-local-security-policy/","publishdate":"2017-08-03T10:44:57+01:00","relpermalink":"/grdnr-live-site/post/2017-08-03-automating-windows-local-security-policy/","section":"post","summary":"Enforcing security policy is tough, especially in a Windows environment where you are NOT using Group Policy. Think about usage patterns for Windows server on cloud. GPO certainly doesn’t always apply.","tags":["chef","windows","security"],"title":"Automating Windows Local Security Policy","type":"post"},{"authors":null,"categories":null,"content":"Deploying infrastructure in the cloud is fun. Deploying into AWS is even more fun. Clicking buttons is not fun. You should use a Cloud Formation template.\n AWS CloudFormation gives developers and systems administrators an easy way to create and manage a collection of related AWS resources, provisioning and updating them in an orderly and predictable fashion.\n Maintaining a Cloud Formation template (hopefully you wrote it in yaml) brings the same challenges as maintaining any code base. How can you verify the quality of code, especially in a shared environment.\nI like to use Travis to kick off simple verification tests on any pull request against my CFT.\n Travis CI is a hosted, distributed continuous integration service used to build and test software projects hosted at GitHub\n It’s a continuous integration tool that can automate the execution of verification tests such as syntax, linting and unit tests, as well as executing deploying your application to a service such as Hroku- although this isn’t relevant in the CFT example.\nHow does it work? After setting up your account you need to enable Travis as an extension for your CFT repo. This is handled through the Settings and then Services panels.\nOnce enabled you need to add a Travis file to your repo. This file controls what Travis does when a certain trigger event occurs, such as a pushed branch or a new pull request.\nThe following example is really simple. I’ll explain it in a minute.\n Numbers = lines\n  I’m using Ruby gems so I need Travis to configure the build node accordingly.\n  I need to configure the AWS CLI on the build node which requires passing through credentials. This is where I set them. Travis can create a secure string for you.\n  Onto the good stuff. Here I’m specifying the steps that Travis needs to take to prepare the build node for executing my required tests. In this case I’m installing the AWS CLI and a YAML linting Ruby gem.\n  This is where we specify the actual commands to run in order to tests the codebase. Here I’m calling the aws cloudformation verify-template command against a path to my template, and the same with the ruby gem.\n  Some nice output :)\n  Notifications are important, especially when tests fail. This is notifying my team’s Slack channel at Chef. I like to turn email off as the GitHub UI + Slack are enough notifications for me.\n  What’s next? Hopefully the above gives you some idea of basic verification tests that Travis can execute. Arguably they are only saving a little bit of time as an attempted deployment with a malformed template will fail anyway. Having said that, I have a nice starting point from which to extend, and getting testing into an IT team’s culture is important.\nThe next step would be to add the template deployment. At least then you can check AMIs exist and a deployment creates. You may wish to follow this with a set of Selenium tests, should your stack deploy an application. Either way, I hope you find the above example useful in your own adventures with CI!\n","date":1501751697,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1501751697,"objectID":"d55b2204168ae4ef6707ccf78888cbdb","permalink":"https://grdnrio.github.io/grdnr-live-site/post/2017-08-03-using-travis-to-verify-a-cloud-formation-template/","publishdate":"2017-08-03T10:14:57+01:00","relpermalink":"/grdnr-live-site/post/2017-08-03-using-travis-to-verify-a-cloud-formation-template/","section":"post","summary":"Deploying infrastructure in the cloud is fun. Deploying into AWS is even more fun. Clicking buttons is not fun. You should use a Cloud Formation template.\n AWS CloudFormation gives developers and systems administrators an easy way to create and manage a collection of related AWS resources, provisioning and updating them in an orderly and predictable fashion.","tags":["travis","cloud","cloud formation","aws"],"title":"Using Travis to verify a Cloud Formation Template","type":"post"},{"authors":null,"categories":null,"content":"Tech Days NL ran on thr 4th and 5th of October 2016 at the RAI centre in Amsterdam. It was a great event with an amazing number of tracks running in parallel. The number of delegates and speakers was truly impressive- great job Microsoft!\nI was fortunate to be invited to speak at the event. An area of increasing importance is the automation of security and compliance so it made sense to speak about how this is possible with InSpec on Microsoft\u0026rsquo;s cloud platform, Azure.\nHere\u0026rsquo;s the video of my talk.\n Talk summary:\n Traditionally, regulatory compliance has been at odds with making changes to systems. As we work in more agile and automated ways, regularly changing our systems and applications, how do we resolve this conflict with the compliance requirements of an organisation? In addition to this, as regulatory frameworks become more complicated to reflect diverse technology environments, how can you reign in that complexity making compliance part of your workflow, instead of an isolated silo? We\u0026rsquo;ll look at how Compliance can be automated in a business, and run through a live example using different platforms running on Azure.\n And here are my slides, if you\u0026rsquo;re into that sort of thing\u0026hellip;\n ","date":1478304000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1478304000,"objectID":"14d33b163bdc2f4f472817df4d57deae","permalink":"https://grdnrio.github.io/grdnr-live-site/post/2016-11-05-inspec-on-azure-at-tech-days-nl-2016/","publishdate":"2016-11-05T00:00:00Z","relpermalink":"/grdnr-live-site/post/2016-11-05-inspec-on-azure-at-tech-days-nl-2016/","section":"post","summary":"Tech Days NL ran on thr 4th and 5th of October 2016 at the RAI centre in Amsterdam. It was a great event with an amazing number of tracks running in parallel.","tags":["chef","inspec","microsoft","talks"],"title":"InSpec on Azure at Tech Days NL 2016","type":"post"},{"authors":null,"categories":null,"content":"InSpec is an opensource language that can be used to assess the state of systems. It can form integration tests, but more importantly, with additional meta information, can create so called compliance profiles. These represent either business requirements or industry standards such as ISO 27001 and CIS.\nOn the 12th October I presented with Christoph Hartmann at the London Chef Summit on the subject of InSpec. We spoke about the changes in InSpec 1.0 and gave a demo showing how you can use InSpec in your cookbooks for integration tests, and to produce compliance profiles that can be applied at all stages of the development process.\nHere are the resources from our talk!\n Resources Simple web cookbook - https://github.com/grdnrio/inspec-summit Here you will see a .kitchen.yml file that contains the runlist for os and ssh hardening taken from the metadat.rb dependencies that we used in our demo. You\u0026rsquo;ll also find the website style attributes in the default location.\nOS and SSH baseline InSpec profiles: Use the following profiles to assess state.\n  OS profile - https://github.com/dev-sec/tests-os-hardening\n  SSH profile - https://github.com/dev-sec/tests-ssh-hardening\n  Example corporate profile This repo shows how you can build a single profile to address all of your compliance scanning needs with InSpec. This example, used in the presentation, shows how you can include upstream profiles, skip controls, and also include your own InSpec tests. It also demonstrates platform awareness, showing how a prpfile can be platform agnostic and therefore applied holistically.\n https://github.com/chris-rock/acme-inspec-profile\nOS and SSH hardening cookbooks:\n  OS hardening - https://github.com/dev-sec/chef-os-hardening\n  SSH hardening - https://github.com/dev-sec/chef-ssh-hardening\n  Finally the best place to check for everything InSpec is the brand new website - inspec.io\n","date":1477094400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1477094400,"objectID":"f3e56ef82adf6bad4044a65efb36bed5","permalink":"https://grdnrio.github.io/grdnr-live-site/post/2016-10-22-inspec-and-chef-compliance-as-code/","publishdate":"2016-10-22T00:00:00Z","relpermalink":"/grdnr-live-site/post/2016-10-22-inspec-and-chef-compliance-as-code/","section":"post","summary":"InSpec is an opensource language that can be used to assess the state of systems. It can form integration tests, but more importantly, with additional meta information, can create so called compliance profiles.","tags":["chef","inspec","community"],"title":"InSpec and Chef - compliance as code","type":"post"},{"authors":null,"categories":null,"content":"The Chef DK includes a tool called Knife. This is primarily used for interaction with remote resources in Chef, such as the Chef Server and Chef managed nodes, but can be used for managing other remote systems too. Here\u0026rsquo;s the description from the Chef Docs.\n knife is a command-line tool that provides an interface between a local chef-repo and the Chef server.\n  https://docs.chef.io/knife.html\nIt\u0026rsquo;s a great tool and a vital part of any Chef administration tasks. Unfortunately Ruby on Windows is slower than Linux and so Windows users sometimes experience performance issues. Check out the result of the following running on my top-spec Ultrabook running Windows 10:\nC:UsersjgardRepositorieschef-repo\u0026gt; Measure-Command {knife node list} Days : 0 Hours : 0 Minutes : 0 Seconds : 6 Milliseconds : 312 Ticks : 63120307 TotalDays : 7.30559108796296E-05 TotalHours : 0.00175334186111111 TotalMinutes : 0.105200511666667 TotalSeconds : 6.3120307 TotalMilliseconds : 6312.0307  A wait of 6 seconds every time I want to run the commonly used knife node list is not ideal. Fortunately a knife command that comes with the Chef DK by default is available to address this issue.\nUsing knife rehash we can create a cache of the local knife sub commands on disk. When knife runs it no longer builds a tree of available sub commands, which reduces execution time.\nC:UsersjgardRepositorieschef-repo\u0026gt; knife rehash Using knife-rehash will speed up knife's load time by caching the location of subcommands on disk. However, you will need to update the cache by running `knife rehash` anytime you install a new knife plugin. Knife subcommands are cached in C:/Users/jgard/.chef/plugin_manifest.json. Delete this file to disable the caching.  After creating the cache the knife node list command performance is greatly improved.\nC:UsersjgardRepositorieschef-repo\u0026gt; Measure-Command {knife node list} Days : 0 Hours : 0 Minutes : 0 Seconds : 2 Milliseconds : 954 Ticks : 29545646 TotalDays : 3.4196349537037E-05 TotalHours : 0.000820712388888889 TotalMinutes : 0.0492427433333333 TotalSeconds : 2.9545646 TotalMilliseconds : 2954.5646  There are some caveats:\n If you install a new Knife gem or plugin you need to re-run knife rehash. The same applies for a chef DK update- don\u0026rsquo;t forget knife rehash! If you get strange output with Knife it\u0026rsquo;s worth trying a knife rehash before debugging further.  Finally, if you want to stop using the cache just delete the JSON file. On Windows it\u0026rsquo;s here: C:/Users/\u0026lt;username\u0026gt;/.chef/plugin_manifest.json\n","date":1475798400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1475798400,"objectID":"508d5d20eb189c275f21970b91b657e8","permalink":"https://grdnrio.github.io/grdnr-live-site/post/2016-10-07-slow-knife-speed-up-with-knife-rehash/","publishdate":"2016-10-07T00:00:00Z","relpermalink":"/grdnr-live-site/post/2016-10-07-slow-knife-speed-up-with-knife-rehash/","section":"post","summary":"The Chef DK includes a tool called Knife. This is primarily used for interaction with remote resources in Chef, such as the Chef Server and Chef managed nodes, but can be used for managing other remote systems too.","tags":["chef","windows"],"title":"Slow Knife? Speed up with Knife rehash","type":"post"},{"authors":null,"categories":null,"content":"Using knife to bootstrap a node to be managed with Chef is a fundamental part of the Chef workflow, especially for dev and test purposes. When you bootstrap a node you are preparing the node to communicate with the Chef Server so it can download the cookbooks and recipes you have defined in its run-list, and eventually match the state you have defined in your Chef code.\nknife bootstrap windows winrm ADDRESS --winrm-user USER --winrm-password 'PASSWORD' --node-name node1 --run-list 'recipe[learn_chef_iis]'\nAs part of the bootstrapping process for Windows the chef-client package is retrieved from the chef.io website. You can see this in the process output.\n54.171.10.153 C:UsersAdministrator\u0026gt;goto install 54.171.10.153 Checking for existing downloaded package at \u0026quot;C:UsersADMINI~1AppDataLocalTempchef-client-latest.msi\u0026quot; 54.171.10.153 No existing downloaded packages to delete. 54.171.10.153 Attempting to download client package using PowerShell if available... 54.171.10.153 powershell.exe -ExecutionPolicy Unrestricted -NoProfile -NonInteractive -File C:chefwget.ps1 \u0026quot;https://www.chef.io/chef/download?p=windows\u0026amp;pv=2012\u0026amp;m=x86_64\u0026amp;DownloadContext=PowerShell\u0026amp;v=12\u0026quot; \u0026quot;C:UsersADMINI~1AppDataLocalTempchef-client-latest.msi\u0026quot; 54.171.10.153 Download via PowerShell succeeded. 54.171.10.153 Installing downloaded client package... 54.171.10.153 54.171.10.153 C:UsersAdministrator\u0026gt;msiexec /qn /log \u0026quot;C:UsersADMINI~1AppDataLocalTempchef-client-msi7958.log\u0026quot; /i \u0026quot;C:UsersADMINI~1AppDataLocalTempchef-client-latest.msi\u0026quot; 54.171.10.153 Successfully installed Chef Client package. 54.171.10.153 Installation completed successfully  If working in a locked down environment, perhaps behind a firewall, this can be problematic. If your node is unable to retrieve a package from the Internet the bootstrapping process will fail.\nThe work around is to use a (currently) undocumented argument in your bootstrap command.\n--msi-url\nThis argument will accept a remote location as well as a local system path. This means you can use an internal package hosting service of some kind, or reference the package location on the node\u0026rsquo;s filesystem; perhaps baked into your images.\nknife bootstrap windows winrm ADDRESS --winrm-user USER --winrm-password 'PASSWORD' --node-name node1 --msi-url C:/tmp/chef-client.msi\nVoila, your locked down instance is bootstrapped.\n","date":1473811200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1473811200,"objectID":"fdb27f7fb34653f0b08dce06c538430c","permalink":"https://grdnrio.github.io/grdnr-live-site/post/2016-09-14-bootstrapping-windows-nodes-behind-a-firewall-with-knife/","publishdate":"2016-09-14T00:00:00Z","relpermalink":"/grdnr-live-site/post/2016-09-14-bootstrapping-windows-nodes-behind-a-firewall-with-knife/","section":"post","summary":"Using knife to bootstrap a node to be managed with Chef is a fundamental part of the Chef workflow, especially for dev and test purposes. When you bootstrap a node you are preparing the node to communicate with the Chef Server so it can download the cookbooks and recipes you have defined in its run-list, and eventually match the state you have defined in your Chef code.","tags":["chef","windows"],"title":"Bootstrapping Windows nodes behind a firewall with Knife","type":"post"},{"authors":null,"categories":null,"content":"Recently I was lucky enough to take part in a \u0026lsquo;hackathon\u0026rsquo; with Chef, Microsoft and a partner company based in Norway. It was a great week working with some amazing people!\nOne of the challenges we faced was automating the installation of .NET 4.6.1; a requirement of the application that was the focus of the week long event. We struggled with automatically determining whether the package should be installed on the system, and then triggering a reboot in the correct order to allow IIS configuration to complete.\nHere\u0026rsquo;s the Windows package resource we used to install .NET originally.\npackage '.NET 4.6.1' do source 'https://download.microsoft.com/download/E/4/1/E4173890-A24A-4936-9FC9-AF930FE3FA40/NDP461-KB3102436-x86-x64-AllOS-ENU.exe' installer_type :custom action :install returns [0, 3010] options '/norestart /passive' timeout 3000 end  As you can see we needed to set the installer type to :custom because we were using a .exe package. In addition to this we passed some options to prevent an automatic reboot interrupting the Chef run, and to prevent user dialogues from interrupting the installation.\nThis worked, but running chef-client again would re-trigger the installation. Additionally, we had issues with IIS configuration (a different recipe) due to .NET installation being our first task. The flagged reboot prevented IIS features from fully enabling.\nWe decided to push the reboot out to the reboot resource with a notification from the Windows package resource.\nreboot '.Net Install' do reason 'Need to reboot after .NET installation' action :nothing end if version_arr[6][:data] != 394_271 package '.NET 4.6.1' do source 'https://download.microsoft.com/download/E/4/1/E4173890-A24A-4936-9FC9-AF930FE3FA40/NDP461-KB3102436-x86-x64-AllOS-ENU.exe' installer_type :custom action :install returns [0, 3010] options '/norestart /passive' notifies :request_reboot, 'reboot[.Net Install]', :immediately timeout 3000 end  We also changed the recipe order in default.rb to make .NET installation the last task; this fixed our reboot flag issue with IIS.\nWith this working we could now trigger a reboot from the successful installation of .NET as the last task in the chef-client run. Using request_reboot in the Reboot resource ensured a complete run before the machine going down.\nWe still had the problem of idempotency. Every time we converged the .NET installation kicked off, even if it was already installed at the correct version. To resolve this we added some logic. We settled on checking the registry for the build release number (matching 4.6.1) to either skip or trigger the windows_package resource.\nreboot '.Net Install' do reason 'Need to reboot after .NET installation' action :nothing end version_arr = registry_get_values('HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\NET Framework Setup\\NDP\\v4\\Full', :x86_64) if version_arr[6][:data] != 394_271 package '.NET 4.6.1' do source 'https://download.microsoft.com/download/E/4/1/E4173890-A24A-4936-9FC9-AF930FE3FA40/NDP461-KB3102436-x86-x64-AllOS-ENU.exe' installer_type :custom action :install returns [0, 3010] options '/norestart /passive' notifies :request_reboot, 'reboot[.Net Install]', :immediately timeout 3000 end end  Here\u0026rsquo;s the complete, working code.\n","date":1468972800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1468972800,"objectID":"a857199ccf53f2aa72e5e2078e74cbbb","permalink":"https://grdnrio.github.io/grdnr-live-site/post/2016-07-20-installing-net-4-6-1-on-windows-server-2012-r2-with-chef/","publishdate":"2016-07-20T00:00:00Z","relpermalink":"/grdnr-live-site/post/2016-07-20-installing-net-4-6-1-on-windows-server-2012-r2-with-chef/","section":"post","summary":"Recently I was lucky enough to take part in a \u0026lsquo;hackathon\u0026rsquo; with Chef, Microsoft and a partner company based in Norway. It was a great week working with some amazing people!","tags":["chef","windows"],"title":"Installing .NET 4.6.1 on Windows Server 2012 R2 with Chef","type":"post"},{"authors":null,"categories":null,"content":"I deliver many presentations over various teleconferencing software from my Mac Book Pro. If you like to use speaker notes in presenter view in Powerpoint then this poses a problem. You have a single screen so when sharing it how can you also view notes?\nYou could aways just add second monitor and share the primary presentation screen, but what if you\u0026rsquo;re on the move or in a call booth? There is a workaround, but it requires an iOS device with iOS 7.0+.\nBy using the Duest Display app you can turn your iOS device into a secondary screen. This is a great app (costing £7.99) that provides you with numerous options for configuring the secondary iOS screen. It\u0026rsquo;s works over WiFi, but can be terribly slow, so just hook your device up to your laptop with the provided cable. Make sure it\u0026rsquo;s the official Apple one- I had issues with a third party cable. Duet also works on Windows ;)\nOnce you\u0026rsquo;re all setup with Duet it\u0026rsquo;s just a case of setting the iOS device (or whichever you prefer) as your secondary screen so that presenter view loads. When you\u0026rsquo;re on a video call share the presentation screen and you will be able to view your notes on the mobile device.\n","date":1467331200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1467331200,"objectID":"a02411faaaccf0617761808e8565f1d7","permalink":"https://grdnrio.github.io/grdnr-live-site/post/2016-07-01-view-speaker-notes-whilst-presenting-over-a-screen-share-on-osx/","publishdate":"2016-07-01T00:00:00Z","relpermalink":"/grdnr-live-site/post/2016-07-01-view-speaker-notes-whilst-presenting-over-a-screen-share-on-osx/","section":"post","summary":"I deliver many presentations over various teleconferencing software from my Mac Book Pro. If you like to use speaker notes in presenter view in Powerpoint then this poses a problem. You have a single screen so when sharing it how can you also view notes?","tags":["osx","mac"],"title":"View speaker notes whilst presenting over a screen share on OSX","type":"post"},{"authors":null,"categories":null,"content":"Posh Git is a really helpful open-source project that includes a number of scripts offering Git and Powershell integration. When installed it allows tab auto completion for common Git operations along with showing the current branch and state of files.\nAs you can see above, showing the current branch is neat.\nInstallation Let\u0026rsquo;s get this bad boy installed.\nFirstly you need to install Git for Windows. Head to the Git page (not GitHub) and download the correct package for your version of Windows. https://git-scm.com/download/win\nOnce the package has downloaded, click through the installation options. I went with the Windows command prompt integration instead of the provided terminal environment.\nNow you should be able to type Git into a Powershell terminal and see that options are spat back out, suggesting that Git has been installed successfully.\nNow we need to edit our security settings before installing PoshGit. Make sure that you\u0026rsquo;re running your Powershell environment as Administrator. If not you can right click on a Powershell shortcut and choose this option.\nWe\u0026rsquo;re going to run a command that allows us to execute remotely signed scripts. Essentially we\u0026rsquo;re switching to Developer mode instead of relying on packages signed by Microsoft or trusted developers.\nRun this command now:\nSet-ExecutionPolicy RemoteSigned\nIt will ask for confirmation, so say yes with Y.\nNow we need to grab the PoshGit repo. It\u0026rsquo;s time to use your shiny new Git command line tools. Before cloning the PoshGit repo it might be helpful to create a Repository or Project directory in your file system. Personally I use C:\\Users\\jgard\\Repositories.\nAfter changing to a suitable directory on your file system, run this command.\ngit clone https://github.com/dahlbyk/posh-git.git\nYou can view the full proect page on GitHub - https://github.com/dahlbyk/posh-git\nNow change to the recently cloned PoshGit directory and run the installer script\ncd posh-git .install.ps1  When the installation completes the installer will tell you that you need to reset your session. Do this with the following command.\n. $PROFILE\nVoila. Post Git in your Powershell environment. One thing to note is that you need to set your Execution Policy for each Powershell environment (x86 and x64 for example).\n","date":1465257600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1465257600,"objectID":"0c4dad1205f895cd8258e7871812129d","permalink":"https://grdnrio.github.io/grdnr-live-site/post/2016-06-07-install-posh-git-on-windows-10-x64/","publishdate":"2016-06-07T00:00:00Z","relpermalink":"/grdnr-live-site/post/2016-06-07-install-posh-git-on-windows-10-x64/","section":"post","summary":"Posh Git is a really helpful open-source project that includes a number of scripts offering Git and Powershell integration. When installed it allows tab auto completion for common Git operations along with showing the current branch and state of files.","tags":["windows","git"],"title":"Install Posh Git on Windows 10 (x64)","type":"post"},{"authors":null,"categories":null,"content":"This guide shows you how to use the varnishadm command in your system crontab to automate Varnish cache purging.\nAccess your server as root or switch to the root user. If you installed Varnish from a repo you will have all the Varnish utility commands installed as well. You need to make sure that varnishadm is installed so run the command now. You will get an output asking you for switches and arguments. This is correct.\nvarnishadm usage: varnishadm [-t timeout] [-S secretfile] -T [address]:port command [...]\nBefore going any further you may wish to read over the varnishadm manual pages, just to familiarise yourself with the command.\nman varnishadm\nAs you can see from the man page you need to specify the host and secret file for issuing varnishadm commands. To purge the cache on local host you can use the following command.\nvarnishadm -T localhost:6082 -S /etc/varnish/secret url.purge .\nLet’s break this command apart.\nvarnishadm -T localhost:6082\nOf course we’re using the varnishadm command. Next we define the host, look down the page for remote purges, for now as this is a local Varnish, localhost is fine. Finally we state the port that Varnish is listening on. You can check the port using the netstat command.\nnetstat -l\nLook for an unrecognised TCP command.\nActive Internet connections (only servers) **tcp 0 0 localhost:6082 _:_ LISTEN**\nNext we have the path to the secret authentication file.\n-S /etc/varnish/secret\nIt is very unlikely that you will have to alter this section.\nFinally we have the actual Varnish purge command. Again you won’t need to change this.\nurl.purge .\nBefore adding the full command to the crontab to automate the task, run it in your terminal. You should see the command executing correctly without throwing any errors. If a connection error does show, check your port number again with netstat.\nIf the command ran correctly you can now add it to your crontab. Copy the command then open the crontab editor (vim) with the following command.\ncrontab -e\nNow press i to enter INSERT mode. You can now paste the command into the crontab.\nYou need to choose how often to execute this command. The options for setting cron execution times are the subject of another guide so either Google it, or search this site for instructions. In the mean time (this is an unrealistic figure) let’s set the cron to run every minute. The cron tab entry will look like this.\n* * * * * varnishadm -T localhost:6082 -S /etc/varnish/secret url.purge .\nNow press Escape, then type :wq. This will write the changes and then quit from crontab editor.\nThat’s it, you’re done. You can stop here if you’re running a local Varnish. If not and you wish to issue cache purge commands from a remote host then keep reading.\nRemote purge commands In order to allow remote connection you need to edit the Varnish config file on your server. Use whichever text editor you prefer.\nvim /etc/varnish/default.vcl\nYou need to copy and paste the following into the default.vcl file.\nacl purge { \u0026quot;*server IP*\u0026quot;; \u0026quot;*123.123.123.123*\u0026quot;; }\nYou need to update the server IP to match the IP of the remote server you will be issuing purge commands from.\nWrite and quit the changes and reload the config.\nservice varnish reload\nNow log in to the server you will be issuing commands from. We’re going to edit the crontab again using the same command as before. The only difference is that we are going to edit the host in the varnishadm command. So, in the crontab (with your own timing settings) paste the following\nvarnishadm -T *varnish-server-IP*:*port-from-netstat* -S /etc/varnish/secret url.purge .\nAs you can see we have changed the IP and port to those of the target Varnish server. The rest of the command is the same.\nIf you have any problems or questions about this guide leave a comment and I will do my best to help you.\n","date":1462924800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1462924800,"objectID":"573ec91d583100ec2c1db5ee049d3798","permalink":"https://grdnrio.github.io/grdnr-live-site/post/2016-05-11-how-to-automate-varnish-cache-purge/","publishdate":"2016-05-11T00:00:00Z","relpermalink":"/grdnr-live-site/post/2016-05-11-how-to-automate-varnish-cache-purge/","section":"post","summary":"This guide shows you how to use the varnishadm command in your system crontab to automate Varnish cache purging.\nAccess your server as root or switch to the root user. If you installed Varnish from a repo you will have all the Varnish utility commands installed as well.","tags":["linux"],"title":"Automate Varnish cache purges","type":"post"},{"authors":null,"categories":null,"content":"This guide shows you the equivalent .htaccess rewrite rules to get WordPress working properly with permalinks on the Nginx web server.\nThere is no direct equivalent for .htaccess in Nginx, so you cannot copy and past the .htaccess rules that WordPress suggests when you change your permalink settings in the dashboard. Instead you need to add the following rules to you site local configuration file.\nYou could put these rules in the global nginx.conf file, generally found at…\n/etc/nginx/nginx.conf\n…but it is much better practice to have a single configuration file with local overrides for each site you are hosting on your Nginx server. Not only does this lead to granular management on a site by site basis, but it also avoids issues that may occur with multiple PHP apps (Drupal, Joomla! etc) each requiring slightly different rewrite rules.\nFirst of all either create or edit the configuration file for your site.\nvim /etc/nginx/sites-available/*website-name*\nThe standard configuration file looks something like this. This is the file used for sudoguides.com\nserver { #Add a server_name entry for each mapped domain server_name sudoguides.com www.sudoguides.com; server_name sudoguides.co.uk www.sudoguides.co.uk; server_name sudoguides.net www.sudoguides.net; access_log /srv/www/sudoguides.com/logs/access.log; error_log /srv/www/sudoguides.com/logs/error.log; root /srv/www/sudoguides.com/public_html; location ~ .php$ { include /etc/nginx/fastcgi_params; fastcgi_pass 127.0.0.1:9000; fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME /srv/www/sudoguides.com/public_html$fastcgi_script_name; } }  Now for a single WordPress installation (not multisite) you need to add the following to this file.\nif (!-e $request_filename) { rewrite ^.* /index.php break; }\nFor a multisite WordPress installation you need to add this.\n# Rewrite for multi site files rewrite /files/(.+)$ /wp-includes/ms-files.php?file=$1 last; # Rewrite for wordpress if (!-e $request_filename) { rewrite ^(.+)$ /index.php?q=$1 last; }\nAfter adding the rules remember to restart Nginx.\nservice nginx restart\nPlease note, after restarting Nginx you may need to reset your permalink settings in the WordPress dashboard. Just reselect the type of permalinks you want and click save again.\nAs usual if you have any problems leave a comment and we will do our best to help you out!\n","date":1462233600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1462233600,"objectID":"e26e4a94aee9b5d36c9d2d2a67af932a","permalink":"https://grdnrio.github.io/grdnr-live-site/post/2016-05-03-wordpress-htaccess-rules-for-nginx/","publishdate":"2016-05-03T00:00:00Z","relpermalink":"/grdnr-live-site/post/2016-05-03-wordpress-htaccess-rules-for-nginx/","section":"post","summary":"This guide shows you the equivalent .htaccess rewrite rules to get WordPress working properly with permalinks on the Nginx web server.\nThere is no direct equivalent for .htaccess in Nginx, so you cannot copy and past the .","tags":["linux","wordpress"],"title":"WordPress .htaccess rules for Nginx","type":"post"},{"authors":null,"categories":null,"content":"A commonly reported problem with the Virgin Media SuperHub is that an SSH connection will timeout. This is apparently due to the device having a very small amount of RAM leading to connection details being dropped.\nIf you are using a Mac or a Linux computer there is a solution, as follows.\nFirst of all bring up a terminal on your respective machine. Then change to you .ssh directory.\ncd /Users/username/.ssh/\nNow create a local config file either using touch or just go straight into vim.\nvim config\nWhen the editor opens copy and paste the following into it. Remember to press i to enter insert mode in vim.\n# Site-wide defaults for various options Host * ServerAliveCountMax 600 ServerAliveInterval 10\nThen save and exit. Hit escape to exit insert mode in vim.\n:wq\nNow it’s probably a good idea to reboot your computer. Next time you try logging in with SSH, leave the terminal idle for a while and see if it times out. You should be fine! I’ve left my terminal open for hours with the connection remaining active.\n","date":1456012800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1456012800,"objectID":"4018a08ef252639fd5bc228d0211350f","permalink":"https://grdnrio.github.io/grdnr-live-site/post/2016-02-21-ssh-timeout-problems-on-a-virgin-media-super-hub-version-2/","publishdate":"2016-02-21T00:00:00Z","relpermalink":"/grdnr-live-site/post/2016-02-21-ssh-timeout-problems-on-a-virgin-media-super-hub-version-2/","section":"post","summary":"A commonly reported problem with the Virgin Media SuperHub is that an SSH connection will timeout. This is apparently due to the device having a very small amount of RAM leading to connection details being dropped.","tags":["linux","ssh"],"title":"SSH Timeout Problems on a Virgin Media Super Hub version 2","type":"post"},{"authors":null,"categories":null,"content":"A couple of months ago I got back to thinking about where I’d like to be, not only career wise, but personally in a few years time. It’s incredible how my vision for the future has matured and developed as life has changed! I came to the conclusion that it was time for a change, with my second child now 8 months old and family life pretty settled, more specifically a career change.\nI’ve been thinking about what I enjoy most about my role as General Manager at CatN: meeting with (potential) clients. More specifically I love that ‘eureka moment’ when the client sees the potential benefits that cloud technology will deliver. I’m a passionate technologist, and bouncing ideas around is great fun!\nSo, I’m delighted to announce that I’ll be leaving CatN and joining hosting behemoth Rackspace as a Cloud Mentor. I’m going to be working as part of the customer success team to design and deliver cloud solutions, but most importantly to unlock the potential of cloud infrastructure and platform in the client’s business and strategic thinking.\nI’m incredibly excited about this new role. The culture at Rackspace is stuff of legend, and I have the opportunity to flex my strategic and tactical muscles in a single role. Most importantly I get to work with some incredibly talented people in a forward thinking, technology leader, and to develop professionally.\nIf you’re reading this and want to learn more about how the Rackspace Managed Cloud can help your business grow, then get in touch (once I’ve been there a couple of weeks!)\n","date":1428105600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1428105600,"objectID":"1c6ecaa5de6921f82d243b344d1409b9","permalink":"https://grdnrio.github.io/grdnr-live-site/post/2015-04-04-an-announcement/","publishdate":"2015-04-04T00:00:00Z","relpermalink":"/grdnr-live-site/post/2015-04-04-an-announcement/","section":"post","summary":"A couple of months ago I got back to thinking about where I’d like to be, not only career wise, but personally in a few years time. It’s incredible how my vision for the future has matured and developed as life has changed!","tags":null,"title":"An announcement!","type":"post"},{"authors":null,"categories":null,"content":"Platform as a Service is the least defined and smallest (annual spend) layer in the “* as a Service stack” despite being a growing market, but this is changing. Venturebeat explain the cloud layers well if you need a refresher – http://venturebeat.com/2011/11/14/cloud-iaas-paas-saas/.\nGraph data from: http://www.idc.com/getdoc.jsp?containerId=prUS24298013\nIaaS and SaaS dominate the market and are well understood. Use Google Apps or Salesforce? Then you’re using SaaS. Bought a virtual machine from a ‘cloud’ supplier? You’re using IaaS! The big gap is PaaS.\nNow we’re seeing the same repeatable pattern of growth and development from the IaaS world in PaaS. Open-source projects are developing existing technologies (see Docker and LXC), then major vendors absorb / package / develop those technologies into (sometimes open source) off the shelf stacks. The big players are RedHat with OpenShift, and Pivotal (spin out from EMC and VMware) with CloudFoundry.\nThink back to a maturing IaaS market before we began the race to the bottom we’re seeing now. If you wanted IaaS or API driven infrastructure then you’re buying public cloud.\nRackspace’s collaboration with NASA resulted in OpenStack, in my view the best private cloud option in the marketplace. It’s made up of multiple components and has become the defacto IaaS solution. Right now Enterprise are seeing it as a cheaper way to run a data centre. Not popular with open source advocates, but an OpenStack implementation can offer massive cost savings when compared with proprietary options – this will change no doubt. OpenStack is, funnily enough, open; there’s no proprietary lock in. It’s flexible, being built on interconnected components (compute, storage etc), and it’s API driven.\nThis maps neatly onto the developing PaaS world, but we’re yet to see a winner (OpenStack has won the IaaS crown, despite what its critics say!). So why is PaaS going to have such a big impact?\nSoftware rules the roost whether it’s SaaS, backend applications, or websites and apps. IaaS (whether private or public) is a great solution for building software on – orchestrating infrastructure within an application puts controls in the developer’s hands. The problem is that there is still a lot of complexity here: networking, storage, backups, automation, deployment, high-availability to name a few. It takes pretty specialist knowledge to build an application solution on IaaS, and the time that developers spend building (and learning how to build) this solution is time not spent on doing what they do best: writing software.\nPaaS allows them to do exactly that. By providing developers with a self contained, pre-configured application environment the complexity of infrastructure management is removed. In a PaaS world IaaS looks increasingly less relevant, unless you are a Service Provider – hello CatN! I believe that that software is a fundamental part of a successful business, PaaS will become a fundamental part of their IT strategy.\nSo what’s the end game? Well there isn’t one! PaaS continues to evolve (see xPaaS – the ‘x’ is variable), and the lines between middleware, integration systems, and application platforms continue to be blurred. As delivery of these technologies change, the way that businesses consume them will change too.\nThe key take away here is to remember why we build build websites and other applications. It’s about engaging with customers to delight them and hopefully retain them. Successful businesses are great at this, and it’s the same reason we’ve seen the rise of Agile an DevOps working methods. Continuous integration, iterative improvements / releases, and digital by default are all at home on PaaS.\n","date":1412985600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1412985600,"objectID":"2cf6c1c3e3b8d0b91f93162983d36f2d","permalink":"https://grdnrio.github.io/grdnr-live-site/post/2014-10-11-paas-will-change-the-face-of-technology-in-business/","publishdate":"2014-10-11T00:00:00Z","relpermalink":"/grdnr-live-site/post/2014-10-11-paas-will-change-the-face-of-technology-in-business/","section":"post","summary":"Platform as a Service is the least defined and smallest (annual spend) layer in the “* as a Service stack” despite being a growing market, but this is changing. Venturebeat explain the cloud layers well if you need a refresher – http://venturebeat.","tags":["cloud","paas"],"title":"PaaS will change the face of technology in business","type":"post"},{"authors":null,"categories":null,"content":"I was recently at a networking event where I got chatting to a founder of an Internet of Things consultancy company. We had very similar views about the confusion around cloud services – at the moment IaaS is the default choice (it doesn’t even have to be explained), but is this the best option?\nThis got me thinking, what are senior decision makers, or buyers, looking for with a move to a cloud platform. If buying decisions are being driven by a CIO then it’s probably going to be cost savings. This makes perfect sense; if you have ephemeral computing requirements then of course you should benefit from the race to the bottom between the big public IaaS providers.\nThe problem is that cost savings don’t naturally deliver business value. Value is one of those intangible things that means different things to different businesses. Moving a load of business services into a public cloud just to save money will almost certainly not deliver value and may in fact result in higher costs. Migration plans never go wrong, right?\nMoving to the cloud is far more of a cultural challenge than technological one.\nI think that to make sure that value (however a business measures this) is derived from what public IaaS can offer all the stake holders need to be on-board. This means engaging with the people carrying out the migration, those running and orchestrating the infrastructure and those making the purchase.\nI hear a lot of confusion around infrastructure being the silver bullet for a business who has a lot of legacy IT, but I think you need to abstract away from the infrastructure to actually get value in the business and that means engaging with your developers and business executives, and making sure that the conversation about cloud is happening across the board.\n","date":1412380800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1412380800,"objectID":"97e56a6ba0c15dccc435a28c9fee37f7","permalink":"https://grdnrio.github.io/grdnr-live-site/post/2014-10-04-moving-to-cloud-iaas-and-paas-is-it-a-silver-bullet/","publishdate":"2014-10-04T00:00:00Z","relpermalink":"/grdnr-live-site/post/2014-10-04-moving-to-cloud-iaas-and-paas-is-it-a-silver-bullet/","section":"post","summary":"I was recently at a networking event where I got chatting to a founder of an Internet of Things consultancy company. We had very similar views about the confusion around cloud services – at the moment IaaS is the default choice (it doesn’t even have to be explained), but is this the best option?","tags":["cloud"],"title":"Moving to cloud IaaS and PaaS - is it a silver bullet?","type":"post"},{"authors":null,"categories":null,"content":"","date":1405728000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1405728000,"objectID":"51578fcf1fe3c461d13bae85b9b00369","permalink":"https://grdnrio.github.io/grdnr-live-site/post/2014-07-19-outside-number-10-downing-street-with-mark-sutton/","publishdate":"2014-07-19T00:00:00Z","relpermalink":"/grdnr-live-site/post/2014-07-19-outside-number-10-downing-street-with-mark-sutton/","section":"post","summary":"","tags":null,"title":"Outside Number 10","type":"post"},{"authors":null,"categories":null,"content":"I made some comments about the challenges that arise when hosting databases, in particular production databases, in a cloud environment. It was interesting to consider where the pain points would be with PaaS and IaaS, and the benefits and draw backs of each option.\n When executing your organisation’s database strategy it is important to consider whether this will be an IaaS or PaaS implementation. A decision between IaaS and PaaS will generally depend on the type of database and the expertise in the business, according to Joe Gardiner, product head at cloud hosting firm CatN\n  “IaaS is becoming more and more commoditised so it can appear an inexpensive option. However, the expertise required to configure a scalable and resilient environment on IaaS for a production database are not to be underestimated and are rarely cheap,” says Gardiner.\n Taken from the article Databases in the cloud – all you need to know on Cloud Pro.\n","date":1393286400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1393286400,"objectID":"0487858a5f9cc8aea9fe8a850709593a","permalink":"https://grdnrio.github.io/grdnr-live-site/post/2014-02-25-comments-on-cloud-pro-databases-in-the-cloud/","publishdate":"2014-02-25T00:00:00Z","relpermalink":"/grdnr-live-site/post/2014-02-25-comments-on-cloud-pro-databases-in-the-cloud/","section":"post","summary":"I made some comments about the challenges that arise when hosting databases, in particular production databases, in a cloud environment. It was interesting to consider where the pain points would be with PaaS and IaaS, and the benefits and draw backs of each option.","tags":["cloud","iaas"],"title":"Comments on Cloud Pro - Databases in the cloud","type":"post"},{"authors":null,"categories":null,"content":"Some points I made about the importance of good code hosted on the cloud were published on Cloud Pro.\n One area that Joe Gardiner, head of product at cloud hosting firm Catn, believes is overlooked is the quality of code used in cloud applications\n  “Let’s face it – good code is better than poor code – especially if that poor code means you are using much more public cloud capacity than you actually need,” he says. “A good project manager should have regular “sanity check” milestones that should highlight whether there is a way to optimise the resource requirements of an application.\n Taken from the article Cloud costs – making sure a move to public IaaS isn’t a financial drain on Cloud Pro.\n","date":1392249600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1392249600,"objectID":"309f1fe65df8da6c1ef20d9e7ab07e0a","permalink":"https://grdnrio.github.io/grdnr-live-site/post/2014-02-13-comments-on-cloud-pro-keeping-iaas-costs-in-check/","publishdate":"2014-02-13T00:00:00Z","relpermalink":"/grdnr-live-site/post/2014-02-13-comments-on-cloud-pro-keeping-iaas-costs-in-check/","section":"post","summary":"Some points I made about the importance of good code hosted on the cloud were published on Cloud Pro.\n One area that Joe Gardiner, head of product at cloud hosting firm Catn, believes is overlooked is the quality of code used in cloud applications","tags":["cloud","iaas"],"title":"Comments on Cloud Pro - Keeping IaaS costs in check","type":"post"},{"authors":null,"categories":null,"content":"I made some comments about selecting a hosting service for an e-commerce website. They were published on Tech Radar.\n Coupled with this is the service level agreement (SLA) that supports the e-commerce servers in use, as Joe Gardiner, head of hosting at CatN, explains: “Make sure you have real people providing expert support. No IT system can be perfect and things will always go wrong at some stage, normally at a peak time. Knowing that you have a responsive, expert team that you can contact 24/7 is vital, especially in e-commerce where a minute of downtime can cost a lot of money.\n  “It’s also important to make sure that the system encompasses integration options, such as secure credit card transactions, credit checks and so on, and that it enables you to migrate away flexibly.”\n Taken from the article How to choose the right server for your e-commerce site on Techradar.com.\n","date":1390176000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1390176000,"objectID":"27f35ebf3d31b0c7aed00b13c1694771","permalink":"https://grdnrio.github.io/grdnr-live-site/post/2014-01-20-comments-on-techradar/","publishdate":"2014-01-20T00:00:00Z","relpermalink":"/grdnr-live-site/post/2014-01-20-comments-on-techradar/","section":"post","summary":"I made some comments about selecting a hosting service for an e-commerce website. They were published on Tech Radar.\n Coupled with this is the service level agreement (SLA) that supports the e-commerce servers in use, as Joe Gardiner, head of hosting at CatN, explains: “Make sure you have real people providing expert support.","tags":["cloud"],"title":"Comments on Tech Radar - Ecommerce hosting","type":"post"},{"authors":null,"categories":null,"content":" I will never stop learning. I won’t just work on things that are assigned to me. I know there’s no such thing as a status quo. I will build our business sustainably through passionate and loyal customers.\n  I will never pass up an opportunity to help out a colleague, and I’ll remember the days before I knew everything. I am more motivated by impact than money, and I know that Open Source is one of the most powerful ideas of our generation. I will communicate as much as possible, because it’s the oxygen of a distributed company. I am in a marathon, not a sprint, and no matter how far away the goal is, the only way to get there is by putting one foot in front of another every day. Given time, there is no problem that’s insurmountable.\n  This was taken from Matt Mullenweg’s personal blog.\n","date":1376697600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1376697600,"objectID":"e46dc18303d43242b0c9c6ca705d7ac1","permalink":"https://grdnrio.github.io/grdnr-live-site/post/2013-08-17-matt-mullenwegs-company-creed/","publishdate":"2013-08-17T00:00:00Z","relpermalink":"/grdnr-live-site/post/2013-08-17-matt-mullenwegs-company-creed/","section":"post","summary":"I will never stop learning. I won’t just work on things that are assigned to me. I know there’s no such thing as a status quo. I will build our business sustainably through passionate and loyal customers.","tags":["government"],"title":"Matt Mullenweg's company creed","type":"post"},{"authors":null,"categories":null,"content":"Ever seen BuzzFeed? Of course you have, BuzzFeed is pretty much the reason for the Internet. The 28 funniest Cat Fails 2013 – need I say more.\nWhy is it so addictive though? Yes the content is funny (most of the time) and the list format makes you want to read more, but what I find myself clicking on again and again is the ‘More Buzz’ link block at the end of every post.\n The BuzzFeed ‘More Buzz’ widget is fiendishly addictive. Go on then, one more post.\nThis is nothing new, loads of websites use related posts widgets or links somewhere in their content. Why is this though? It’s pretty simple – if you make your money from a CPM or CPC ad model then it’s all about keeping that bounce rate low, and keeping the page per visit rate and visit duration high. This is the best way to get the most value out of each visitor. You’ve already got them to the site, that’s the hard bit, now get as many page views (and potentially ad clicks) as possible out of them.\nThis got me thinking, could I improve the bounce rate on my own websites: The Urbanaut and Shit London?\nA plugin Both websites are built on WordPress so I decided to look for a plugin to add this functionality to the site. This is one of the great benefits of WordPress, a non-coder like me can quickly and easily add features to their website.\nThe best plugin I found was, unsurprisingly, called ‘ WordPress Related Posts‘. It’s dead simple and is sensitive to mobile devices helpfully. After installing it the widget appeared at the end of each blog post.\n The related posts plugin working well but looking poor without post titles.\nUnfortunately we don’t use titles on Shit London (soon changing) only image captions so the plugin doesn’t look too good yet. I’ve set aside a lot of time over the next couple of weeks to go through all 600ish posts and add a title. Here you can see the same plugin on The Urbanaut looking much better, picking up the post titles.\n Related posts being generated dynamically from the same category. Looking goof with post titles being captured correctly.\nI’m much happier with this and hope to achieve the same with Shit London.\nAnalytics This is how the bounce rates look at the moment on both sites. This data is from a 15 day period.\nThe Urbanaut  \nNot bad but not great. I could definitely try and improve the pages per visit and the bounce rate. The duration isn’t too bad considering this is mainly a photo website. It looks as though the content is getting read.\nShit London  \nI’m happy with these figures. The page per visit rate is impressive and the duration is very good for a website of this type. I’m not resting on my laurels though, there is definitely room for improvement.\nWhat next? I’m going to follow up in a months time to see if there has been any improvement in these figures having installed a related posts plugin. The plugin itself provides an analytics system tracking plugin page views and CTR so I will report these figures too. Fingers crossed that we have a good month and see improvements across the board!\n","date":1375401600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1375401600,"objectID":"e483805b5b6758f977b95907b66a908f","permalink":"https://grdnrio.github.io/grdnr-live-site/post/2013-08-02-you-might-also-like/","publishdate":"2013-08-02T00:00:00Z","relpermalink":"/grdnr-live-site/post/2013-08-02-you-might-also-like/","section":"post","summary":"Ever seen BuzzFeed? Of course you have, BuzzFeed is pretty much the reason for the Internet. The 28 funniest Cat Fails 2013 – need I say more.\nWhy is it so addictive though?","tags":["seo","content","marketing"],"title":"You might also like","type":"post"},{"authors":null,"categories":null,"content":"I have compiled a list of mandatory documents by looking through the ISO/IEC 27001:2005 standard and by pulling together online resources I am currently relying on for my own work towards ISO 27001 accreditation in my current role at CatN.\nThe Documents TitleDescriptionISMS Scope  A document (or set of) which lists all areas of the business that are covered by the ISMS. This might include company assets, IT systems, locations and software used. ISMS Statement  Documentation approved by management which specifies the objective(s) of the ISMS and the requirements it will satisfy. Procedures supporting the ISMS  These are technical documentation, security information, system designs and existing procedures in the company which support the ISMS. Risk Assessment Methods  Describe how you determine the impact and likelihood of risks in the business. There are plenty of online resources to help you do this. Risk Assessment Report  A collection of documents outlining the risks identified using the methods described in the above document, and any outcomes and action that may need to be taken following the assessment. Risk Treatment Plan  Essentially a project plan describing how the objectives of the ISMS described in the ISMS Statement are achieved. International Standard Records  These are similar to the above but mandatory documents required by the standard. Electronic security logs and information security such as ID badges. ISMS Operational Records  A collection of documents recording and describing the procedures in the company that are in place to ensure that the ISMS Statement objectives continue to be achieved. Often this may be HR and Recruitment processes, NDA agreements etc. Metrics need to be defined to enable tracking of the ISMS performance. Statement of Applicability  Consolidated results from the risk assessment. It should also state the ISMS **control** objectives, the objectives of the systems in place to control the ISMS following the risk assessment and treatment. Document Control Systems  A documented procedure for controlling access to and changes to records in the business and records used towards the ISO 27001 standard. Normally a documented classification system of some kind is used. Management  There need to be records detailing the experience, education, qualifications and any other relevant information for all members of staff who will be accessing and managing the ISMS. You may also want to include staff evaluation reports to track employee behaviour. Prevention  There should be an understanding of potential issues when attempting to conform with the ISMS, and this understanding should be documented with suggested controls to prevent breaches of ISMS policy. Correction  Similar to prevention yo should be able to demonstrate that there is a plan in place to follow on from preventative measures in the case of the standards of the ISMS being breached. Detail how you will correct and resolve any breaches or issues. Company ISMS Audits  There should be documented procedures and plans for carrying out audits of the ISMS and all relevant documentation on a regular basis. The results of the audits should be stored so that external auditing requirements are more easily met and so that managers can review the reports when required.  The Check List I created a public Google Doc spreadsheet for the check list. Copy and paste it into your own doc if you want and if you any ideas about how I could improve it then let me know. The spreadsheet link is below.\n Mandatory ISO27001 Document Check List\nReference As a final note, if you’re looking into ISO27001 accreditation to aid with BIL* accreditation for Central Government work, this is an excellent blog describing the process and requirements: CESG IL2/IL3 Accreditation.\n","date":1368316800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1368316800,"objectID":"345e7b544c6ad8ab6c50678495a92b6f","permalink":"https://grdnrio.github.io/grdnr-live-site/post/2013-05-12-iso-27001-mandatory-documents/","publishdate":"2013-05-12T00:00:00Z","relpermalink":"/grdnr-live-site/post/2013-05-12-iso-27001-mandatory-documents/","section":"post","summary":"I have compiled a list of mandatory documents by looking through the ISO/IEC 27001:2005 standard and by pulling together online resources I am currently relying on for my own work towards ISO 27001 accreditation in my current role at CatN.","tags":["government","security"],"title":"ISO 27001 Mandatory Documents","type":"post"},{"authors":null,"categories":null,"content":"Ruby as a programming language has always interested me, partly because it is such a high level language! It’s also interesting that everything is an object in Ruby essentially scrapping that jump from scripting to OO programming, which I never made.\nThere is a wealth of knowledge (as with any major language) online, however I thought I would put together a list of the most useful tutorials I have found as a complete Ruby n00b to aid others who may be just getting started.\nRoadmap for learning Rails  http://techiferous.com/2010/07/roadmap-for-learning-rails/\nOK, so technically not a Ruby tutorial, but this post by Wyatt Greene puts forth a compelling argument for starting with Ruby and not jumping straight to Rails. Well worth a read for a grounding in today’s essential web technologies.\nRuby on Rails Development With Mac OS X Mountain Lion  http://createdbypete.com/blog/2012/08/ruby-on-rails-development-with-mac-osx-mountain-lion.html\nThis isn’t the bets title in the world to describe the content of the post, and again it isn’t actually Rails specific. This guide describes how to set up a Ruby development environment on Mac OSX. A great way to get started quickly, as Ruby isn’t like PHP where you have a local LAMP dev server, instead you can run it in your command line using IRB.\nA Quick (and Hopefully Painless) Ride Through Ruby (with Cartoon Foxes)  http://mislav.uniqpath.com/poignant-guide/book/\nThe title says it all… This is a really good online tutorial which teaches the concepts and constructs behind Ruby in an amusing way. At moments this book is seriously crazy – read it out loud and you’ll see what I mean. Well worth reading for comedy value alone, but also a really interesting way of introducing Ruby programming concepts.\nRuby for Newbies (series)  http://net.tutsplus.com/tutorials/ruby/ruby-for-newbies-installing-ruby-and-getting-started/\nNet Tuts can almost always be relied on to provide quality content. This Ruby is guide is no different and is part 1 in a series of 13 videos and tutorials! Solid tutorial and a great starting point.\nRuby on Codeacademy  http://www.codecademy.com/tracks/ruby\nIf you prefer a more hands on approach over screen casts and written tutorials then check out Codeacademy. It’s already a well known website, but what sets it apart is the browser based terminal where you will be challenged to complete tasks and immediately tested on the quality of your code. The best thing? It’s free!\nUpdate\nCore Ruby Tutorial  http://rubylearning.com/satishtalim/tutorial.html\nThanks to Sunil Kelkar for suggesting this great resource. It’s one of the older Ruby courses on the web and is an awesome summary of the Ruby language taking you from data types to arrays.\n I’ll keep adding new tutorials as I discover them, and if you have any that you can recommend let me know.\n","date":1363910400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1363910400,"objectID":"470a1a2184333fbffc99e2df7e8ddf28","permalink":"https://grdnrio.github.io/grdnr-live-site/post/2013-03-22-the-best-ruby-tutorials-for-a-n00b/","publishdate":"2013-03-22T00:00:00Z","relpermalink":"/grdnr-live-site/post/2013-03-22-the-best-ruby-tutorials-for-a-n00b/","section":"post","summary":"Ruby as a programming language has always interested me, partly because it is such a high level language! It’s also interesting that everything is an object in Ruby essentially scrapping that jump from scripting to OO programming, which I never made.","tags":["ruby"],"title":"The Best Ruby Tutorials (for a n00b)","type":"post"},{"authors":null,"categories":null,"content":"Product Development is a difficult discipline to define. I think that the role differs very slightly in every type of business. It is often confused with project management, however I see them as very different disciplines.\nFor me a product manager is a number 2. The company founder is a number one, creating those kernels of ideas, the founder of a start-up. The product manager should take that kernel and make it happen – it is as simple as that.\nDon’t get me wrong – product managers should be creative, understand their market and propose new feature development and of course many will invent entirely new products which will disrupt. However I believe that a good product manager should be a second in a business and should act as a hub, pulling all the threads inside a business together to create.\nThe process I have found a real lack of resource online explaining at a high level the product development process. There are plenty of guides describing ‘creating’ a new product in the entrepreneurial sense, however developing a product is very different.\nThe following is the process that I go through as a product manager. I explain each of the steps in greater detail below.\nEDIT: This is a great article explaining the need for customer research at an early stage in the planning process: ‘ How user research can help prioritise product requirements‘.\nBrainstorm This is the starting point for any project. Bring together all your evidence, this could be market research, customer stories, bug reports or all of the above and start sketching out some ideas. You need to crystallise your idea before engaging stake holders, especially senior ones.\nDiscuss Now is the time to engage with your key stake holders. Take the result of your brainstorming and your evidence and get buy in. This may be the CEO, Product Director or MD – remember to provide as much evidence as possible.\nRoadmap With buy in you can start to describe what you want to develop at a high level and plan milestones for the development process from your perspective as a Product Manager. It is good to use visuals at his point, however you can use a spreadsheet or one of the various tools available to do this. You can start to predict a launch date at this point but remember that it will change following the technical planning stage. The roadmap is so important because it allows stake holders to gain a high level understanding of your development plan at any time. Essentially it is a great point of reference for the whole business.\nTechnical Planning Time to engage with your technical teams, generally software developers and engineers. Review your roadmap with your engineers and begin to break apart your high level targets into individual development goals. At this point engineers may suggest new issues you hadn’t thought of so be flexible. You need to work with the tech teams to predict the number of man hours it will take to complete each goal. This will help you work up a deadline.\nPrioritise Bearing in mind the technical requirements group development goals into features vital for launch and features you can release later. Take the lean approach and work out what you can release quickly and which tasks may need to be broken down more. Your target is to create a feature list for launch based on the time estimates and business requirements you should understand at this point.\nStories You’re almost at the development stage but before getting to work go back to your customer stories. It is good to reassess the original reasoning behind your product planning to make sure that the plan and roadmap you have created eventually lead to features and a product which will actually solve customer stories. If not, you need to rethink and go back to your technical planning stage to re-prioritise.\nDevelopment Hopefully you’re happy that you will anser some of your customer stories with your first code release so get the devs working to the plan. Make sure that you track progress and make a metric available to judge performance. Time is a good one… set targets and track progress and whether or not objectives are met.\n","date":1360022400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1360022400,"objectID":"d944854ec75b42ecc80b1fe3be184c0e","permalink":"https://grdnrio.github.io/grdnr-live-site/post/2013-02-05-product-development-process/","publishdate":"2013-02-05T00:00:00Z","relpermalink":"/grdnr-live-site/post/2013-02-05-product-development-process/","section":"post","summary":"Product Development is a difficult discipline to define. I think that the role differs very slightly in every type of business. It is often confused with project management, however I see them as very different disciplines.","tags":["product"],"title":"Product Development Process","type":"post"},{"authors":null,"categories":null,"content":"It’s been a busy few weeks!\nI’ve been moving a project that I’ve been working on for about a year now to new hosting, and migrating the site from a single WordPress to a multisite. What project is it? Well, you may have heard of it! It’s the Shit London series of websites and books.\nThe project was started by my cousin, Patrick Dalton, as a Facebook group where users could upload and share photos of the grimey under belly of London. The photos were often hilarious, and the group absolutely took off, so much so that it demanded a dedicated blog which is where I stepped in.\nAbout 12 months ago the Shit London book came out featuring some of the better images submitted to the website and Facebook group. It was a roaring success, gracing coffee tables all over the world leading to the start of a series of books and websites. The first of this series was the Rude London book, published about 6 months ago, and at the time of writing Patrick is in New York working on material for a book there, and will soon be heading down under for a Shit Australia version too!\nEach new book requires a new website so I made the decision to move each individual shit WordPress site to a single WordPress multisite on a dedicated Web VPS, with a dedicated remote MySQL host.\nIt has taken me a while to get it up and running, mainly due to struggling with Nginx rewrite rules, but I’m happy to say the sites are all in one place now and easily managed. Using Nginx (and caching) has also got the page load time nice and low, about 1 second from the UK for Shit London.\nSo, what are these sites? Well you can access them on the links below\n  Shit London  Shit Universe  Shit NYC  Many more will be coming soon!\n","date":1352419200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1352419200,"objectID":"533eeda38f61948cdb043faff2358184","permalink":"https://grdnrio.github.io/grdnr-live-site/post/2012-11-09-the-shit-network/","publishdate":"2012-11-09T00:00:00Z","relpermalink":"/grdnr-live-site/post/2012-11-09-the-shit-network/","section":"post","summary":"It’s been a busy few weeks!\nI’ve been moving a project that I’ve been working on for about a year now to new hosting, and migrating the site from a single WordPress to a multisite.","tags":null,"title":"The Shit Network","type":"post"},{"authors":null,"categories":null,"content":"I have had a few questions posted on a previous post, “ Setting up port forwarding on a Virgin Media SuperHub with a Zyxel NSA310 NAS” that ask about how to connect to the NAS remotely after setting up port forwarding. This post is a quick guide for using the FileZilla FTP CLIENT to do this.\nFileZilla is cross-platform compatible so you can follow this guide using the FileZilla client on Linux, Mac OSX, or Windows.\n Head to the FileZilla Client download page and select the option most suitable for your computer. Take note if you are a Windows user, it is easier to go with the Setup.exe version which you can download directly here. Also, OSX users make sure you choose PowerPC or Intel depending on your system. When the client is downloaded install it on your system. I’m not going to tell you how to do that… If you do get stuck there is a documentation section on the FileZilla website. Now you need to find out what your IP address is as assigned to you by your ISP. You can do this by going to Google and searching “My IP”. At the top of the results your IP address will be shown! Take a note of this, I would copy and paste it into a text file for later use. Launch the FileZilla client you just installed. For now all you need to worry about is the bar at the top of the window that has the Host, Username, and Password fields. We are going to fill these out now to connect to your NAS.   \n In the Host field enter the IP address you just got from Google.   In the Username field enter either the admin username, or the user you created in the web interface or setup utility for your NAS. In the Password field enter the password you chose for the admin account or the password you chose for the user you created.  N.B. If you do not know how to create a user or have not done so refer to your NAS documentation, or access the web interface locally (at home) for your NAS and follow the user wizard in the Administration section.\n You can leave the port field blank, unless you have chosen a different FTP port when you set up port forwarding instead of the default which I suggested. If you have done this then you probably don’t need this guide! When all the details have been added you can click Quick Connect to attempt a connection to your NAS. If successful you will see the right hand side of the FileZilla window populate with the directories on your NAS. You can now browse around them as if it were any other filesystem on your computer. To download files you can drag them from the right hand panel to the left hand one which is your local filesystem of the computer you are using.  Troubleshooting If you are unable to connect there may be one of the following problems.\n  You may have restarted your router since following this guide and your ISP may have assigned you a new IP address. If this is the case, head back to Google, find out your IP address again and try connecting with the same username and password as before. Note, every time you reset your router you will be assigned a new IP address by your ISP and you need top update your FTP connection settings accordingly.\n  Your username and password information may be wrong. If this is the case FileZilla will tell you that either the username or password was wrong so it is pretty easy to track down this problem. Head back to your local NAS web interface and change the user settings accordingly!\n  FileZilla is unable to connect. There may be a problem with your port forwarding configuration. Read my previous guide, “ Setting up port forwarding on a Virgin Media SuperHub with a Zyxel NSA310 NAS“, and carefully go over all the steps again. If you still have problems leave a comment on either guide and I will try to help.\n  FileZilla very quickly fails to connect. This is likely to be a firewall issue on the computer you are attempting to connect with. Do a quick Google search for, “opening FTP outbound on a \u0026lt;_insert operating system here_\u0026gt; firewall” and follow the instruction.\n  Good luck, and if you have any problems (or just want to say thanks ;)) leave a comment!\n","date":1348012800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1348012800,"objectID":"ff414eb4c7d75ef06548365086cf5495","permalink":"https://grdnrio.github.io/grdnr-live-site/post/2012-09-19-remotely-connecting-to-your-nas-with-filezilla-ftp-client/","publishdate":"2012-09-19T00:00:00Z","relpermalink":"/grdnr-live-site/post/2012-09-19-remotely-connecting-to-your-nas-with-filezilla-ftp-client/","section":"post","summary":"I have had a few questions posted on a previous post, “ Setting up port forwarding on a Virgin Media SuperHub with a Zyxel NSA310 NAS” that ask about how to connect to the NAS remotely after setting up port forwarding.","tags":null,"title":"Remotely connecting to your NAS with FileZilla FTP client","type":"post"},{"authors":null,"categories":null,"content":"Cloning a hard drive is an especially useful thing to do if you wish to replace your existing hard drive. I recently did this and upgraded from my 7.2K 1TB Western Digital drive to a 120GB OCZ Agility 3 SSD (it was absolutely worth it by the way!).\nThis is how I did it…\nI used a really useful tool called Reflect. It is quite tricky to find the free version of it these days, but to help you out here is a link: Macrium Reflect Free Edition. I’ll do my best to keep the link updated…\nWhen the download is complete install the software. I tested for viruses and it is clean. It also seemed to run very smoothly on my Windows 7 workstation. Fire the program up and you’ll see that it scans your drives to work what you currently have connected to your computer.\nNow you need to connect the drive which you wish to clone to (the new drive) to your computer. You could do this by opening your computer up and connecting it internally, or you could use a USB external drive mounter, something like this: Startech External Hard Drive Dock. Either way attach the device and power your computer back on, if you shut it down.\nWhen it comes back open up My Computer to see if the new drive is detected. If it isn’t you may need to format it and create a simple volume. To do this follow the guide, “Creating a simple volume on a new hard drive in Windows 7”. If the drive is there you can go back to the Reflect program you installed and open it up.\nYour hard drives, including the new one, will be listed when the program opens up. Left click on the drive you wish to clone, then click on the Clone this disk button underneath the drive. This will start up the disk cloning wizard.\n \nNow select a destination drive to clone to, this will be your new drive. Choose your new drive from the list that appears after clicking the ‘Select a disk to clone to’ link. When the drive is selected click Next. This will begin the cloning process. Follow the rest of the options through to complete the disk cloning.\nDepending on the size of the drive and how it is connected to your computer it make take around an hour.\nYou can now open up My Computer and view the contents of the drive. It should be exactly the same as the source drive that you cloned. Now if you are cloning a drive for backups you can stop here, however if you are cloning a drive in order to replace an old drive, you will need to change drive letters in order for your installed programs to continue to work correctly running from the cloned drive.\nGo to Start button -\u0026gt; All Programs -\u0026gt; Administrative Tools -\u0026gt; Computer Management. Then Storage, and Disk Management.\n \nRight click on the blue volume block of the disk you cloned from (the source) and from the options menu select Change drive letter and paths.\nNow click on the Change button.\n \nThen choose the new drive letter. I went with X for clarity when doing this.\n \nNow repeat the process for your newly created clone drive (destination drive) but choose the original drive letter of the destination drive, probably C or D.\nThe Disk Manager will ask you to restart your computer for changes to take effect; do this now. When the computer comes back online you should see that the drive letters have swapped over. You can shut your computer down again and remove the old drive from inside the computer.\nNow when you turn the computer back on again you will be able to run any programs and access data that may have been on a secondary drive without any problems. A good example for me was Steam; games were installed on my D drive which I cloned to a new SSD.\n","date":1343433600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1343433600,"objectID":"0ec04a14bb82aeed5b7604a2528255c9","permalink":"https://grdnrio.github.io/grdnr-live-site/post/2012-07-28-cloning-a-hard-drive-in-windows-7/","publishdate":"2012-07-28T00:00:00Z","relpermalink":"/grdnr-live-site/post/2012-07-28-cloning-a-hard-drive-in-windows-7/","section":"post","summary":"Cloning a hard drive is an especially useful thing to do if you wish to replace your existing hard drive. I recently did this and upgraded from my 7.2K 1TB Western Digital drive to a 120GB OCZ Agility 3 SSD (it was absolutely worth it by the way!","tags":["windows"],"title":"Cloning a hard drive in Windows 7","type":"post"},{"authors":null,"categories":null,"content":"Like many other I have just purchased an iPad 3, and yes the retina display is very beautiful. Having never owned an iOS device before I have spent the majority of the last two weeks spending a fortune on the App Store. Whilst populating my iPad with useless apps, I too a look at the availability of apps for writing code, preferably with an option for SFTP access to a remote server.\nThe selection is rather limited, however the following are the best options that I found.\nKoder   This app is unrelated to Coda on OSX! It’s a decent IDE, but seems buggy when trying to connect with SFTP. Standard FTP is fine but rather slow. The inerface is the best of all the options, although on the retina display it does looks a little pixelated occasionally.\nFor me though the FTP access really was too slow, and the app would occasionally crash to the app screen, very annoying if you are in the middle of typing. As soon as the purchase shows up in my history I will be requesting a refund.\nGusto   Again, Gusto offers FTP and SFTP access to your remote server. The home screen allows you to add projects similar to the OSX Coda, pulling in a live preview of your projects URL. The interface is decent enough and the performance of the app is pretty good, FTP is certainly faster than Koder.\nThe app looks good on the retina display, and setting up a connection is intuitive and easy to do. The only issue I found was that I couldn’t see a way of importing a private SSH key to allow SFTP connections. In the end I settled for FTP and ensured nothing sensitive was transferred!\nTextastic   It gets great reviews and with reason. The interface is fast, and easy to navigate, and the syntax highlighting is excellent. Koder also support DropBox integration, but the app with the best performance is Textastic easily. The File Properties option makes it an incredibly useful tool, particularly for checking details such as line count.\nThe one down side is that DropBox doesn’t sync automatically, downloading and re-uploading is a manual process, however FTP syncing is effortless and very easy to setup.\nConclusion You guessed it, Textastic is my favourite. All the apps are similarly priced the last time I checked so if you’re happy to spend around £7.00 on an iPad IDE, my money is with Textastic.\n","date":1341792000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1341792000,"objectID":"eeb4c800c22b725b29841a9c7826b6e7","permalink":"https://grdnrio.github.io/grdnr-live-site/post/2012-07-09-coding-app-on-the-ipad/","publishdate":"2012-07-09T00:00:00Z","relpermalink":"/grdnr-live-site/post/2012-07-09-coding-app-on-the-ipad/","section":"post","summary":"Like many other I have just purchased an iPad 3, and yes the retina display is very beautiful. Having never owned an iOS device before I have spent the majority of the last two weeks spending a fortune on the App Store.","tags":["apple"],"title":"Coding app on the iPad","type":"post"},{"authors":null,"categories":null,"content":"The standard method for removing app icons from the Lion Launchpad is to hold the left mouse button over the app icon, wait for the cross to appear in the corner of the app icon, and then click it!\nIn some cases however app icons can become orphaned, for example if you use on Steam on Mac, deleting local content to uninstall a game will not remove the game icon from Launchpad. This leaves you with an unusable orphaned icon that seems invincible and does not appear in the Application folder along with the rest of your apps.\nThe solution is as follows…\n Open Launchpad, find the offending icon(s) and drag them to your normal dock. Right click on the icon that you dragged to your dock and choose ‘Show in Finder’. Now drag the icon(s) from the Finder windows that opens up into the Trash. Finally right click on the dock item(s) that you dragged originally and choose ‘Remove from dock’.  Voila, the icons have disappeared from Launchpad and the annoying orphaned icons that seemed invincible have been thwarted.\n","date":1339632000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1339632000,"objectID":"3ee44483010007788884bb6d8e82655f","permalink":"https://grdnrio.github.io/grdnr-live-site/post/2012-06-14-force-removing-icons-from-launchpad/","publishdate":"2012-06-14T00:00:00Z","relpermalink":"/grdnr-live-site/post/2012-06-14-force-removing-icons-from-launchpad/","section":"post","summary":"The standard method for removing app icons from the Lion Launchpad is to hold the left mouse button over the app icon, wait for the cross to appear in the corner of the app icon, and then click it!","tags":["apple"],"title":"Force removing icons from Launchpad","type":"post"},{"authors":null,"categories":null,"content":"Installing VMware tools in a rpm based guest can sometimes be tricky. Here is a quick and dirty guide to help you install VMware tools and get the most out of your Linux guest.\nGetting Started Of course the first thing you will need is a VMware software product (depending on your OS) capable of running guests. I use a Mac Book Air circa 2011 running Lion so I have VMware Fusion installed. You can get it from here if you also run a Mac: VMware Fusion. If you’re running Windows or Linux have a look at their products page to select the best option for you.\nSurprisingly you will then need to install a RPM based Linux guest. How to do that goes beyond the scope of this guide, I will presume you are able to do this. I may write a guide at a later date, but their are loads of tutorials online and in the VMware help guides.\n Boot up your Linux VM and wait until you get to a stable desktop, if you are using one. If you installed your distro without a desktop environment you can still use the following commands (although you probably won’t need this guide if you prefer a CLI only OS!).\nWhen the OS is booted, have a look at the top of your VMware app window. There should be a toolbar, you want the tab called Virtual Machine. In this menu select ‘Install VMware Tools‘. When you click this it will mount an ISO with the VMware tools package on it.\n \nFirst switch to root.\nsu -\nYou need to set the CD at a mount point so that you can copy over the VMware Tools files.\nmount /dev/cdrom /mnt\nNow copy the files to your Desktop.\ncp /mnt/VMwareTools-* /home/user/Desktop\nRemember to use Tab to auto complete the VMware Tools version, and change user with your own username.\nChange to your Desktop so that you can extract the VMware Tools archive.\ncd /home/user/Desktop\nNow extract the archive.\ntar -xzvf VMwareTools-*\nChange to the extracted folder in order to run the VMware Tools installer.\ncd vmware-tools-distrib/\nRun the installer.\n./vmware-install.pl\nYou will now be asked a lot of questions by the installer. Generally you can go with the default settings by pressing Enter and everything will install correctly. The advanced settings of the installer are beyond the scope of this guide!\n \nWhen the installer completes reboot your VM, either using the VMware Player / Fusion / Workstation settings, or with the following command.\nreboot\nWhen your Linux VM comes back online you will have all the advanced features of VMware tools such as directory sharing, clipboard sharing, graphics adapter interfacing, automatic resolution adjustment etc.\nFinally to remove those left over files on your Desktop. You will need to be root again.\nsu -\nChange to the Desktop.\ncd /home/user/Desktop/\nThen delete the files.\nrm -f -R VMwareTools-* vmware-tools-distrib/\nAnd you’re finished! Enjoy your Linux VM with VMware Tools!\n","date":1338076800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1338076800,"objectID":"a4120cfe35bd8acbfe768ef0b1b203c1","permalink":"https://grdnrio.github.io/grdnr-live-site/post/2012-05-27-installing-vmware-tools-in-a-centos-scientific-linux-red-hat-guest/","publishdate":"2012-05-27T00:00:00Z","relpermalink":"/grdnr-live-site/post/2012-05-27-installing-vmware-tools-in-a-centos-scientific-linux-red-hat-guest/","section":"post","summary":"Installing VMware tools in a rpm based guest can sometimes be tricky. Here is a quick and dirty guide to help you install VMware tools and get the most out of your Linux guest.","tags":["linux"],"title":"Installing VMware Tools in a Centos / Scientific Linux / Red Hat guest","type":"post"},{"authors":null,"categories":null,"content":"I have a lot of films / music / software / games stored in various places and on various devices in my home. Keeping all this data organised is an absolute nightmare, and with a recent string of disk failures, I’ve also lost quite a lot of it. Following the recent death of an external USB HD I was in the market for a newer model. Looking around it made sense to invest a little more in a NAS to centralise storage on my home network and offer the redundancy of RAID 1 (across two disks, the 4 bay NASs are a tad outside my budget!).\nThe device I selected is the ZyXEL NSA310, mainly because it was a good price, but also because it has an SMB, FTP and NFS server built in, as well as a decently reviewed web admin panel. Setting it up on my network was incredibly simple. In the box is a setup utility disk that when installed, offers a ‘Quick Start’ wizard to take you through the basic steps to configure the NAS. This utility is Windows only, however it isn’t really required after the intial config.\nThe NSA310 has a gigabit port in the back so my initial heavy data transfer went quite quickly at around 30 (peaking at 40) Mbps. I then setup the FTP server with a few users to allow external access. This was all completed through the web admin panel (when logged in as the admin user)…\n \nThe only complication was to allow external network access to the FTP server was setting up port forwarding on my rubbish Virgin Media Super Hub. Setting up forwarding rules was actually very simple, but that doesn’t stop the Super Hub from being rubbish.\n Log into your Hub by IP (probably 192.168.0.1) and enter you login details. Click on the ‘Advanced Settings’ link at the bottom of the page. Under the advanced title in the sidebar, click on the Port Forwarding option. This will load the control panel for forwarding. Now you just need to fill out the fields to create a rule…  Name: You can call your rule anything you like. I called it NAS FTP. Start Port -\u0026gt; End Port: Enter 20 for the start port and 21 for the end port. These are the two ports FTP most commonly uses. Protocol: The protocol is TCP. Local IP Address: This is the IP of NAS on your network, in my case 192.168.0.100. Click Add, and then log out and you’re finished.  Now you can FTP with the FTP user or admin user you created at your ISP IP and the FTP traffic will automatically be forwarded to your NAS by your router.\nUPDATE: I have written a follow up guide for connecting to your NAS using the FileZilla FTP cleint, it is available at, \u0026ldquo; Remotely connecting to your NAS\u0026rdquo;.\n","date":1335916800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1335916800,"objectID":"b5ca3667afbebfb46b223936f0c05d46","permalink":"https://grdnrio.github.io/grdnr-live-site/post/2012-05-02-external-ftp-nas-with-the-zyxel-nsa310-media-storage-and-a-super-hub/","publishdate":"2012-05-02T00:00:00Z","relpermalink":"/grdnr-live-site/post/2012-05-02-external-ftp-nas-with-the-zyxel-nsa310-media-storage-and-a-super-hub/","section":"post","summary":"I have a lot of films / music / software / games stored in various places and on various devices in my home. Keeping all this data organised is an absolute nightmare, and with a recent string of disk failures, I’ve also lost quite a lot of it.","tags":["linux"],"title":"External FTP NAS with the ZyXEL NSA310 Media Storage and a Super Hub","type":"post"},{"authors":null,"categories":null,"content":"My new Seagate Momentus XT hard drive turned up today! I bought it to breath new life into my old Toshiba laptop, and so far the signs are good. The Momentus XT is a standard 320GB 7,200 rpm disk, but it has an additional 4GB cache that remembers the files you use the most. The idea is that commonly used files are accessed on the flash memory, massively improving performance over the spindle element of the disk.\nI don’t have any specific benchmarks, but my Ubuntu install seems to boot a little quicker. The real noticeable performance is the speed in which the desktop becomes stable and disk access stops. This may because BIOS etc, keep the boot at a consistent time, but once Ubuntu begins to load it really is faster.\nMy next test will be starting disk intensive applications such as Eclipse to check the boot times of these. This will be the real test and will hopefully help me decide if the invesment was really worth it. The only downside to installing this disk is that it is a tiny bit noisier, and some users have reported errors with power saving. There are also reported issues when running VMs. Apparently the Hypervisor will report corrupt data on some occassions. Fortunately these errors are most common on Mac and PC, so hopefully my resilient Ubuntu install will shrug any potential problems off.\nIt looks as though there’s not much else I can do to help my old Tosh fight off the years. The next upgrade will be a new box I think, maybe one of these…\n \n","date":1333756800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1333756800,"objectID":"8da9860f5bfde01b59e193c45dbd77d0","permalink":"https://grdnrio.github.io/grdnr-live-site/post/2012-04-07-new-momentus-xt-hard-drive/","publishdate":"2012-04-07T00:00:00Z","relpermalink":"/grdnr-live-site/post/2012-04-07-new-momentus-xt-hard-drive/","section":"post","summary":"My new Seagate Momentus XT hard drive turned up today! I bought it to breath new life into my old Toshiba laptop, and so far the signs are good. The Momentus XT is a standard 320GB 7,200 rpm disk, but it has an additional 4GB cache that remembers the files you use the most.","tags":["linux"],"title":"New Momentus XT Hard Drive","type":"post"}]